{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPClassifier - Wisconsin breast cancer dataset\n",
    "Python code that splits the original Wisconsin breast cancer dataset into two subsets: training/validation (80%), and test (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X_breast_cancer = breast_cancer.data\n",
    "y_breast_cancer = breast_cancer.target\n",
    "breast_cancer_target_names = breast_cancer.target_names\n",
    "\n",
    "# Dataset split: 68% training, 12% validation and 20% test.\n",
    "\n",
    "# Setting 'random_state' to my any number, in this case my A number(20548919) makes sure that the splits are repeatable.\n",
    "# X_tv and y_tv are temporary variables; tv stands for training and validation.\n",
    "# As only one split can be done at a time, first let's split to 80% and 20%\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X_breast_cancer, y_breast_cancer, test_size=0.2, random_state=20548919)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code that uses an additional split to create a validation dataset or Python code that implements a cross-validation approach to tune the MLP model hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size:  569\n",
      "Training set size:  386  ( 68.0 % )\n",
      "Validation set size:  69  ( 12.0 % )\n",
      "Test set size:  114  ( 20.0 % )\n"
     ]
    }
   ],
   "source": [
    "# Splitting the temporary set into training and validation sets (15% validation and the rest for training => \n",
    "# From entire dataset 12% validation and 68% training.\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_tv, y_tv, test_size=0.15, random_state=20548919)\n",
    "\n",
    "# Sizes of each dataset (just to see if the split was done correctly)\n",
    "print(\"Original size: \", X_breast_cancer.shape[0])\n",
    "print(\"Training set size: \", X_train.shape[0], \" (\", np.round(100*X_train.shape[0]/X_breast_cancer.shape[0]), ('% )'))\n",
    "print(\"Validation set size: \", X_validation.shape[0], \" (\", np.round(100*X_validation.shape[0]/X_breast_cancer.shape[0]), ('% )'))\n",
    "print(\"Test set size: \", X_test.shape[0], \" (\", np.round(100*X_test.shape[0]/X_breast_cancer.shape[0]), ('% )'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code that uses MLPClassifier to train, validate and test an MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2 score:  0.9455958549222798\n",
      "Validation R^2 score:  0.9130434782608695\n",
      "Test R^2 score:  0.9298245614035088 \n",
      "\n",
      "Classification report:\n",
      " \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.85      0.91        46\n",
      "      benign       0.91      0.99      0.94        68\n",
      "\n",
      "    accuracy                           0.93       114\n",
      "   macro avg       0.94      0.92      0.93       114\n",
      "weighted avg       0.93      0.93      0.93       114\n",
      "\n",
      "Well-predicted example: \n",
      " [2.425e+01 2.020e+01 1.662e+02 1.761e+03 1.447e-01 2.867e-01 4.268e-01\n",
      " 2.012e-01 2.655e-01 6.877e-02 1.509e+00 3.120e+00 9.807e+00 2.330e+02\n",
      " 2.333e-02 9.806e-02 1.278e-01 1.822e-02 4.547e-02 9.875e-03 2.602e+01\n",
      " 2.399e+01 1.809e+02 2.073e+03 1.696e-01 4.244e-01 5.803e-01 2.248e-01\n",
      " 3.222e-01 8.009e-02] \n",
      " Ground truth (Actual label): 0 \n",
      " Predicted label: 0 \n",
      "\n",
      "Poorly-predicted example: \n",
      " [1.625e+01 1.951e+01 1.098e+02 8.158e+02 1.026e-01 1.893e-01 2.236e-01\n",
      " 9.194e-02 2.151e-01 6.578e-02 3.147e-01 9.857e-01 3.070e+00 3.312e+01\n",
      " 9.197e-03 5.470e-02 8.079e-02 2.215e-02 2.773e-02 6.355e-03 1.739e+01\n",
      " 2.305e+01 1.221e+02 9.397e+02 1.377e-01 4.462e-01 5.897e-01 1.775e-01\n",
      " 3.318e-01 9.136e-02] \n",
      " Ground truth (Actual label): 0 \n",
      " Predicted label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training the model\n",
    "# MLPClassifier is initialized with modified hyperparameters\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', alpha=0.001, random_state=20548919, max_iter=125, solver=\"lbfgs\")\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Score of the final model for each of the datasets: training, validation and test\n",
    "# Training set\n",
    "training_accuracy = mlp_classifier.score(X_train, y_train)\n",
    "# Validation set\n",
    "validation_accuracy = mlp_classifier.score(X_validation, y_validation)\n",
    "# Test set\n",
    "test_accuracy = mlp_classifier.score(X_test, y_test)\n",
    "# Outputs\n",
    "print(\"Training R^2 score: \", training_accuracy)\n",
    "print(\"Validation R^2 score: \",validation_accuracy)\n",
    "print(\"Test R^2 score: \", test_accuracy, \"\\n\")\n",
    "\n",
    "# Initialization of prediction of the model in the test dataset\n",
    "y_test_prediction = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Classification report for the test set to see how well the model predicts the classes in the test data\n",
    "print(\"Classification report:\\n\", \"\\n\", classification_report(y_test, y_test_prediction, target_names = breast_cancer_target_names))\n",
    "\n",
    "# Well predicted example\n",
    "# Initialization\n",
    "index_well = int(-1) \n",
    "# Loop and compare to see if the prediction is equal to the ground truth\n",
    "for i in range(len(y_test)):\n",
    "    ground_truth = y_test[i]\n",
    "    prediction = y_test_prediction[i]\n",
    "    if ground_truth == prediction:  \n",
    "        # Save index if it is the same\n",
    "        index_well = i\n",
    "        # Stop looking for more (just one example needed)\n",
    "        break\n",
    "\n",
    "# Poorly predicted example\n",
    "# Initialization\n",
    "index_poor = int(-1) \n",
    "# Loop and compare to see if the prediction is different to the ground truth\n",
    "for i in range(len(y_test)):\n",
    "    ground_truth = y_test[i]\n",
    "    prediction = y_test_prediction[i]\n",
    "    # Check if the predicted value does NOT match the actual value\n",
    "    if ground_truth != prediction:\n",
    "        # Save index if it is different\n",
    "        index_poor = i\n",
    "        # Stop looking for more (just one example needed)\n",
    "        break\n",
    "\n",
    "print(\"Well-predicted example:\", \"\\n\", X_test[index_well], \n",
    "      \"\\n\", \"Ground truth (Actual label):\", y_test[index_well], \n",
    "      \"\\n\",  \"Predicted label:\", y_test_prediction[index_well],\"\\n\")\n",
    "print(\"Poorly-predicted example:\", \"\\n\", X_test[index_poor], \"\\n\", \n",
    "      \"Ground truth (Actual label):\", y_test[index_poor], \"\\n\", \n",
    "      \"Predicted label:\", y_test_prediction[index_poor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPRegressor - Diabetes dataset\n",
    "\n",
    "The above process should be repeated for the Diabetes dataset using MLPRegressor. Shuffle and split the original dataset into training/validation (80%) and test (20%) sets. Be sure to use the \"random_state\" input, so we can recreate the same split when testing your code. Then, develop a documented process to determine a set of hyperparameters that do the best job of predicting the targets for the examples in the validation set. You may create a separate validation set or use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size:  442\n",
      "Training set size:  300  ( 68.0 % )\n",
      "Validation set size:  53  ( 12.0 % )\n",
      "Test set size:  89  ( 20.0 % )\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Loading the diabetes data\n",
    "diabetes = load_diabetes()                                      \n",
    "X_diabetes = diabetes.data                              # n samples\n",
    "y_diabetes = diabetes.target                            # disease progression\n",
    "diabetes_features_names = diabetes.feature_names        # name of the features\n",
    "\n",
    "# Dataset split: 68% training, 12% validation and 20% test.\n",
    "\n",
    "# Setting 'random_state' to my any number, in this case my A number(20548919) makes sure that the splits are repeatable.\n",
    "# X_tv and y_tv are temporary variables; tv stands for training and validation.\n",
    "# As only one split can be done at a time, first let's split to 80% and 20%\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X_diabetes, y_diabetes, test_size=0.2, random_state=20548919)\n",
    "# Splitting the temporary set into training and validation sets (15% validation and the rest for training => \n",
    "# From entire dataset 12% validation and 68% training.\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_tv, y_tv, test_size=0.15, random_state=20548919)\n",
    "# Sizes of each dataset (just to see if the split was done correctly)\n",
    "print(\"Original size: \", X_diabetes.shape[0])\n",
    "print(\"Training set size: \", X_train.shape[0], \" (\", np.round(100*X_train.shape[0]/X_diabetes.shape[0]), ('% )'))\n",
    "print(\"Validation set size: \", X_validation.shape[0], \" (\", np.round(100*X_validation.shape[0]/X_diabetes.shape[0]), ('% )'))\n",
    "print(\"Test set size: \", X_test.shape[0], \" (\", np.round(100*X_test.shape[0]/X_diabetes.shape[0]), ('% )'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_base.py:173: RuntimeWarning: overflow encountered in square\n",
      "  return ((y_true - y_pred) ** 2).mean() / 2\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1005: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_base.py:173: RuntimeWarning: overflow encountered in square\n",
      "  return ((y_true - y_pred) ** 2).mean() / 2\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1005: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_base.py:173: RuntimeWarning: overflow encountered in square\n",
      "  return ((y_true - y_pred) ** 2).mean() / 2\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1005: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_base.py:173: RuntimeWarning: overflow encountered in square\n",
      "  return ((y_true - y_pred) ** 2).mean() / 2\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1005: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [ 4.47539596e-001 -4.22132299e-002  4.52160076e-001 -3.25258137e-001\n",
      "  4.52149998e-001 -4.06257310e-002  4.43356878e-001 -3.38635612e-002\n",
      "  4.37502572e-001 -3.60375908e+000  4.32706966e-001 -1.69660523e-002\n",
      "  4.45090946e-001 -5.50255757e+004  4.37102790e-001 -5.17320820e+064\n",
      "  3.98329783e-001 -1.43586817e+071  4.21479156e-001 -4.89667659e-001\n",
      "  4.21436585e-001 -1.03401379e+027  3.89222769e-001 -1.12411693e+093\n",
      "  4.50521947e-001 -5.07495176e-002  4.36805045e-001 -1.14330323e+004\n",
      "  3.78702918e-001 -1.03052906e+078  4.22473040e-001 -1.22214935e+031\n",
      "  4.43695042e-001 -9.63922846e+046  4.46734736e-001 -6.67402468e+025\n",
      "  4.57314926e-001 -5.23998583e+144  4.47334567e-001             -inf\n",
      "  4.67043765e-001 -6.54479135e+182  4.47539685e-001 -4.22211746e-002\n",
      "  4.52160665e-001 -2.52253121e+000  4.52150751e-001 -4.06257742e-002\n",
      "  4.43356590e-001 -3.38635865e-002  4.38089118e-001 -4.84407451e+000\n",
      "  4.36179409e-001 -1.69660521e-002  4.49067800e-001 -5.50199341e+004\n",
      "  4.54107106e-001 -5.17240267e+064  4.04277734e-001 -1.43514405e+071\n",
      "  4.36756039e-001 -4.91163106e-001  4.45063948e-001 -1.03381809e+027\n",
      "  4.50737182e-001 -1.12390709e+093  4.54188702e-001 -9.10257192e-001\n",
      "  4.36753852e-001 -1.14350882e+004  3.80301939e-001 -1.03012098e+078\n",
      "  4.23620679e-001 -1.22198494e+031  4.14600993e-001 -9.63737150e+046\n",
      "  4.09772232e-001 -6.67373896e+025  4.53900877e-001 -5.23954635e+144\n",
      "  4.50879036e-001             -inf  4.46749990e-001 -6.54378275e+182\n",
      "  4.47540097e-001 -4.22557475e-002  4.52163278e-001 -2.68266840e+000\n",
      "  4.52153803e-001 -2.55012964e-002  4.43356692e-001 -3.38636983e-002\n",
      "  4.37524954e-001 -5.10134856e+000  4.31384139e-001 -1.86010911e-002\n",
      "  4.46370650e-001 -5.49947599e+004  4.68181583e-001 -5.16882402e+064\n",
      "  3.82304695e-001 -1.43193006e+071  4.06551590e-001 -4.91131136e-001\n",
      "  4.34693478e-001 -1.03294875e+027  4.36705418e-001 -1.12297496e+093\n",
      "  4.54273957e-001 -1.43889775e+000  4.40544963e-001 -1.14433709e+004\n",
      "  3.96395393e-001 -1.02830923e+078  4.22711648e-001 -1.22125446e+031\n",
      "  4.18250562e-001 -9.62912264e+046  3.99993430e-001 -6.67246922e+025\n",
      "  4.63598234e-001 -5.23759356e+144  4.32426267e-001             -inf\n",
      "  4.18942482e-001 -6.53930195e+182  4.47540577e-001 -4.22975303e-002\n",
      "  4.52166536e-001 -2.68265551e+000  4.52157607e-001 -6.32476785e-002\n",
      "  4.43356224e-001 -3.38638371e-002  4.28083528e-001 -5.10132858e+000\n",
      "  4.36209733e-001 -4.55031036e-002  4.45825135e-001 -5.49631729e+004\n",
      "  4.69682569e-001 -5.16435411e+064  4.07326633e-001 -1.42792257e+071\n",
      "  4.09274195e-001 -7.42588631e-001  4.35322495e-001 -1.03186310e+027\n",
      "  4.31687921e-001 -1.12181085e+093  4.54238004e-001 -1.43888461e+000\n",
      "  4.36777742e-001 -1.14535119e+004  3.96887195e-001 -1.02604894e+078\n",
      "  4.14704369e-001 -1.22034197e+031  4.18278555e-001 -9.61882132e+046\n",
      "  3.97380426e-001 -6.67088237e+025  4.63985423e-001 -5.23515359e+144\n",
      "  4.52821396e-001             -inf  4.34209935e-001 -6.53370518e+182\n",
      "  4.50157900e-001  1.81508643e-001  2.49641616e-001 -2.32090853e+023\n",
      "  9.04211032e-002 -3.84712789e+041  4.58353711e-001  1.31257733e-001\n",
      "  5.69450855e-002 -3.41917905e+040  3.87403961e-002 -3.83419102e+055\n",
      "  3.75971080e-001 -7.64469898e-002 -2.17995454e-002 -7.56783065e+024\n",
      " -3.39929341e-002 -4.43124807e+040  3.54838136e-001 -4.80367667e-002\n",
      " -2.45032862e-002 -5.32749049e+022 -2.68408106e-002 -8.04379726e+039\n",
      "  3.00803369e-001 -7.28190424e-002 -2.74984908e-002 -3.20183284e+040\n",
      " -2.11076250e-002 -1.68239749e+057  3.27682716e-001 -2.55647292e-001\n",
      " -3.04340251e-002 -1.95396240e+040 -2.53135649e-002 -2.33355120e+057\n",
      " -2.20232645e-002 -5.24048972e-002 -2.23889088e-002 -8.12787797e+024\n",
      " -3.16751499e-002 -2.47627269e+042  4.50156290e-001  1.15004659e-001\n",
      "  2.93733201e-001 -2.32094578e+023 -1.02647830e-002 -3.84717815e+041\n",
      "  4.58357651e-001  1.26975207e-001  5.79478974e-002 -3.41919954e+040\n",
      "  7.33838562e-002 -3.83420875e+055  3.66932588e-001 -7.64469623e-002\n",
      " -2.17995231e-002 -7.56792542e+024 -3.39928235e-002 -4.43127339e+040\n",
      "  3.48557098e-001 -4.80367106e-002 -2.45032805e-002 -5.32746398e+022\n",
      " -2.68407105e-002 -8.04426955e+039  3.38130591e-001 -1.04242060e-001\n",
      " -2.74984349e-002 -3.20175921e+040 -2.11076168e-002 -1.67901765e+057\n",
      "  3.38892588e-001 -2.11062164e-001 -3.04339711e-002 -1.97008043e+040\n",
      " -2.53135470e-002 -2.33196316e+057 -2.20232656e-002 -5.24050199e-002\n",
      " -2.23888978e-002 -8.12801680e+024 -3.16753071e-002 -2.47630698e+042\n",
      "  4.50149163e-001  1.52229841e-001  2.68453204e-001 -2.32110896e+023\n",
      "  4.42746107e-002 -3.84740158e+041  4.58374938e-001  9.83993149e-002\n",
      "  6.60539356e-002 -3.41929058e+040  1.16523406e-001 -3.83428737e+055\n",
      "  3.96025793e-001 -7.64468399e-002 -2.17994237e-002 -7.56834663e+024\n",
      " -3.39923322e-002 -4.43138613e+040  3.48593430e-001 -4.80364613e-002\n",
      " -2.45032553e-002 -5.32734665e+022 -2.68402658e-002 -8.04637372e+039\n",
      "  3.35115941e-001 -1.04361979e-001 -2.74981865e-002 -3.20142595e+040\n",
      " -2.11075795e-002 -1.86918428e+057  3.32408041e-001 -2.12902370e-001\n",
      " -3.04337308e-002 -1.80474926e+040 -2.53134668e-002 -2.30249509e+057\n",
      " -2.20232702e-002 -5.24055649e-002 -2.23888492e-002 -8.12863388e+024\n",
      " -3.16757158e-002 -2.47645932e+042  4.50140343e-001  2.11077815e-001\n",
      "  2.86462391e-001 -2.32131290e+023  1.18278448e-001 -3.84768087e+041\n",
      "  4.58396115e-001  7.63772419e-002  1.47678786e-001 -3.41940438e+040\n",
      "  1.22182280e-001 -3.83438567e+055  3.78149095e-001 -7.64466869e-002\n",
      " -2.17992996e-002 -7.56887312e+024 -3.39917182e-002 -4.43152703e+040\n",
      "  3.70124049e-001 -4.80361496e-002 -2.45032237e-002 -5.32720110e+022\n",
      " -2.68397100e-002 -8.04901613e+039  3.02408243e-001 -1.04360379e-001\n",
      " -2.74978761e-002 -3.20099597e+040 -2.11075318e-002 -1.86923771e+057\n",
      "  3.56805039e-001 -2.12900683e-001 -3.04334304e-002 -1.80803892e+040\n",
      " -2.53133660e-002 -2.34227097e+057 -2.20232757e-002 -5.24062463e-002\n",
      " -2.23887884e-002 -8.12940529e+024 -3.16758026e-002 -2.47664965e+042]\n",
      "  warnings.warn(\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:987: RuntimeWarning: invalid value encountered in subtract\n",
      "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
      "/Users/siqiangwu/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:987: RuntimeWarning: overflow encountered in square\n",
      "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the search the best hyperparameters are:  {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.05, 'max_iter': 10000, 'solver': 'adam'}\n",
      "Training + Validation dataset size:  353  ( 80.0 % )\n",
      "Training R^2 score:  0.5769625411595452\n",
      "Validation R^2 score:  0.5623033269742508\n",
      "Test R^2 score:  0.5570637791269027 \n",
      "\n",
      "Well-predicted example: \n",
      " [ 0.04897352  0.05068012  0.08864151  0.08728655  0.03558177  0.02154596\n",
      " -0.02499266  0.03430886  0.06605067  0.13146972] \n",
      " Ground truth (Actual label): 310.0 \n",
      " Predicted label: 309.3141682205302 \n",
      " Error: 0.6858317794698223 \n",
      "\n",
      "Poorly-predicted example: \n",
      " [ 0.04170844  0.05068012 -0.03854032  0.05285804  0.07686035  0.11642994\n",
      " -0.03971921  0.07120998 -0.02251653 -0.01350402] \n",
      " Ground truth (Actual label): 253.0 \n",
      " Predicted label: 106.49909223620975 \n",
      " Error: 146.50090776379025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Cross-validation\n",
    "parameter_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (50, 100), (100, 100), (50, 50, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.005, 0.01],\n",
    "    'max_iter': [10000],\n",
    "    'learning_rate_init': [0.01, 0.05, 0.1]\n",
    "}\n",
    "# Initialize regressor with random state\n",
    "mlp_regressor = MLPRegressor(random_state = 20548919)\n",
    "# Search for best performing set of hyperparameters\n",
    "clf = GridSearchCV(mlp_regressor, parameter_grid, cv=5, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "# Show the best hyperparameters found\n",
    "print(\"From the search the best hyperparameters are: \", clf.best_params_)\n",
    "\n",
    "# Score of the final model for each of the datasets: training, validation and test\n",
    "\n",
    "# First the model with the best hyperparameters must be trained.\n",
    "# As the best hyperparameters have been found and the validation dataset will not be used again,\n",
    "# the model will be trained on the combined dataset of training and validation (best practises).\n",
    "# best_estimator_ from GridSearchCV finds the model that performed best\n",
    "best_hyperparameters_model = clf.best_estimator_\n",
    "print(\"Training + Validation dataset size: \", X_tv.shape[0], \" (\", np.round(100*X_tv.shape[0]/X_diabetes.shape[0]), ('% )'))\n",
    "best_hyperparameters_model.fit(X_tv, y_tv)\n",
    "# Training set\n",
    "training_accuracy = best_hyperparameters_model.score(X_train, y_train)\n",
    "# Validation set\n",
    "validation_accuracy = best_hyperparameters_model.score(X_validation, y_validation)\n",
    "# Test set\n",
    "test_accuracy = best_hyperparameters_model.score(X_test, y_test)\n",
    "# Outputs\n",
    "print(\"Training R^2 score: \", training_accuracy)\n",
    "print(\"Validation R^2 score: \",validation_accuracy)\n",
    "print(\"Test R^2 score: \", test_accuracy, \"\\n\")\n",
    "\n",
    "# Well predicted and poorly predicted examples\n",
    "# Initialization\n",
    "index_well = -1\n",
    "index_poor = -1\n",
    "min_error = float('inf')  # Initialize minimum error to a large value\n",
    "max_error = -float('inf') # Initialize maximum error to a small value\n",
    "\n",
    "y_test_prediction = best_hyperparameters_model.predict(X_test)\n",
    "\n",
    "# Error margin for a well-predicted example (This is an arbitrary number)\n",
    "error_margin = 5\n",
    "\n",
    "# Loop to find well-predicted and poorly-predicted examples\n",
    "for i in range(len(y_test)):\n",
    "    ground_truth = y_test[i]\n",
    "    prediction = y_test_prediction[i]\n",
    "    error = abs(ground_truth - prediction)\n",
    "    # Check for well-predicted example within the permissible error margin\n",
    "    if error <= error_margin and index_well == -1:  # Also ensure only the first match is saved\n",
    "        index_well = i\n",
    "    # Check for the poorly-predicted example with the largest error\n",
    "    if error > max_error:\n",
    "        max_error = error\n",
    "        index_poor = i\n",
    "\n",
    "# Display the well-predicted example\n",
    "print(\"Well-predicted example:\", \"\\n\",\n",
    "      X_test[index_well], \"\\n\",\n",
    "      \"Ground truth (Actual label):\", y_test[index_well], \"\\n\",\n",
    "      \"Predicted label:\", y_test_prediction[index_well], \"\\n\",\n",
    "      \"Error:\", abs(y_test[index_well] - y_test_prediction[index_well]), \"\\n\")\n",
    "\n",
    "# Display the poorly-predicted example\n",
    "print(\"Poorly-predicted example:\", \"\\n\",\n",
    "      X_test[index_poor], \"\\n\",\n",
    "      \"Ground truth (Actual label):\", y_test[index_poor], \"\\n\",\n",
    "      \"Predicted label:\", y_test_prediction[index_poor], \"\\n\",\n",
    "      \"Error:\", abs(y_test[index_poor] - y_test_prediction[index_poor]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
